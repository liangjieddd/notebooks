[TOC]

# 损失函数

> 监督机器学习问题无非就是“minimizeyour error while regularizing your parameters”，也就是在规则化参数的同时最小化误差。

## 概述

​		损失函数（loss function）是用来**估量模型的预测值f(x)与真实值Y的不一致程度**，它是一个非负实值函数,通常使用L(Y, f(x))来表示，损失函数越小，模型的鲁棒性就越好。损失函数是**经验风险函数**的核心部分，也是**结构风险函数**重要组成部分。模型的结构风险函数包括了经验风险项和正则项，通常可以表示成如下式子：

![img](https://images2018.cnblogs.com/blog/1192699/201806/1192699-20180623120336580-830249688.png)

​		其中，前面的均值函数表示的是经验风险函数，L代表的是损失函数，后面的 Φ 是正则化项（regularizer）或者叫惩罚项（penalty term），它可以是L1，也可以是L2，或者其他的正则函数。整个式子表示的意思是**找到使目标函数最小时的θ值**。

​		最小化误差是为了让我们的模型**拟合**我们的训练数据，而规则化参数是**防止我们的模型过分拟合**我们的训练数据。多么简约的哲学啊！因为参数太多，会导致我们的模型复杂度上升，容易过拟合，也就是我们的训练误差会很小。

​		但训练误差小并不是我们的最终目标，**我们的目标是希望模型的测试误差小，也就是能准确的预测新的样本。**

​		所以，我们需要保证模型“简单”的基础上**最小化训练误差**，这样得到的参数才具有好的泛化性能（也就是测试误差也小），而模型“简单”就是通过规则函数来实现的。有很多种规则化方法，如何选择呢？

一般来说，监督学习可以看做最小化下面的目标函数： 

![img](file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml12952\wps1.jpg) 

其中，第一项L(yi,f(xi;w)) 衡量我们的模型（分类或者回归）对第i个样本的预测值f(xi;w)和真实的标签yi之前的误差。

因为我们的模型是要拟合我们的训练样本的嘛，所以我们要求这一项最小，也就是要求我们的模型尽量的拟合我们的训练数据。

但正如上面说言，我们不仅要保证训练误差最小，我们更希望我们的模型测试误差小，所以我们需要加上第二项，也就是对**参数w的规则化函数Ω(w)**去约束我们的模型尽量的简单。

 

## 常见的loss函数：

![img](file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml12952\wps2.jpg) 

![img](file:///C:\Users\ADMINI~1\AppData\Local\Temp\ksohtml12952\wps3.jpg) 

## 一、LogLoss对数损失函数（逻辑回归，交叉熵损失）

​		有些人可能觉得逻辑回归的损失函数就是平方损失，其实并不是。**平方损失函数可以通过线性回归在假设样本是高斯分布的条件下推导得到**，而逻辑回归得到的并不是平方损失。

​		在逻辑回归的推导中，它假设样本服从**伯努利分布（0-1分布）**，然后求得满足该分布的似然函数，接着取对数求极值等等。

​		而逻辑回归并没有求似然函数的极值，而是把极大化当做是一种思想，进而推导出它的经验风险函数为：**最小化负的似然函数（即max F(y, f(x)) —> min -F(y, f(x)))**。从损失函数的视角来看，它就成了log损失函数了。

**log损失函数的标准形式**：

![img](https://images2018.cnblogs.com/blog/1192699/201806/1192699-20180623131608760-1581173013.png)

​		刚刚说到，取对数是为了方便计算极大似然估计，因为在MLE（最大似然估计）中，直接求导比较困难，所以通常都是**先取对数再求导找极值点**。

​		损失函数L(Y, P(Y|X))表达的是样本X在分类Y的情况下，使概率P(Y|X)达到**最大值**（换言之，**就是利用已知的样本分布，找到最有可能（即最大概率）导致这种分布的参数值；或者说什么样的参数才能使我们观测到目前这组数据的概率最大**）。

​		因为log函数是单调递增的，所以logP(Y|X)也会达到最大值，因此在前面加上负号之后，最大化P(Y|X)就等价于最小化L了。

​		逻辑回归的P(Y=y|x)表达式如下（为了将类别标签y统一为1和0，下面将表达式分开表示）：

![img](https://images2018.cnblogs.com/blog/1192699/201806/1192699-20180623132940057-2092485671.png)

　　将它带入到上式，通过推导可以得到logistic的损失函数表达式，如下：

![img](https://images2018.cnblogs.com/blog/1192699/201806/1192699-20180623133130474-1239399796.png)

　　逻辑回归最后得到的目标式子如下：

![img](https://images2018.cnblogs.com/blog/1192699/201806/1192699-20180623133215671-1349575400.png)

​		上面是针对二分类而言的。这里需要解释一下：**之所以有人认为逻辑回归是平方损失，是因为在使用梯度下降来求最优解的时候，它的迭代式子与平方损失求导后的式子非常相似，从而给人一种直观上的错觉**。

这里有个PDF可以参考一下：[Lecture 6: logistic regression.pdf](https://www.cs.berkeley.edu/~russell/classes/cs194/f11/lectures/CS194 Fall 2011 Lecture 06.pdf).

　　**注意：softmax使用的即为交叉熵损失函数，binary_cossentropy为二分类交叉熵损失，categorical_crossentropy为多分类交叉熵损失，当使用多分类交叉熵损失函数时，标签应该为多分类模式，即使用one-hot编码的向量。**

### 1.二分类交叉熵损失sigmoid_cross_entropy

 ![img](https://img-blog.csdn.net/20180623225513783?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzE0ODQ1MTE5/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

### 2.二分类平衡交叉熵损失balanced_sigmoid_cross_entropy

​		该损失也是用于2分类的任务，相比于sigmoid_cross_entrop的优势在于引入了平衡参数 ，可以进行正负样本的平衡，得到比sigmoid_cross_entrop更好的效果。

![img](https://img-blog.csdn.net/20180623225426459?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzE0ODQ1MTE5/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

### 3.多分类交叉熵损失softmax_cross_entropy

![img](https://img-blog.csdn.net/20180623225727826?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzE0ODQ1MTE5/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

### 4.Focal loss：

​		focal loss为凯明大神的大作，主要用于解决多分类任务中样本不平衡的现象，可以获得比softmax_cross_entropy更好的分类效果。

![img](https://img-blog.csdn.net/20180623225948890?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzE0ODQ1MTE5/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

论文中α=0.25，γ=2效果最好。

 ![img](https://img-blog.csdn.net/20180623230012367?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzE0ODQ1MTE5/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

### 5.合页损失hinge_loss：

​		也叫铰链损失，是svm中使用的损失函数。

​		由于合页损失优化到满足小于一定gap距离就会停止优化，而交叉熵损失却是一直在优化，所以，通常情况下，交叉熵损失效果优于合页损失。

![img](https://img-blog.csdn.net/20180623230158249?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzE0ODQ1MTE5/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

### 6.KL散度：

​		KL散度( Kullback–Leibler divergence)，也叫相对熵，是描述两个概率分布P和Q差异的一种方法。它是非对称的，这意味着D(P||Q) ≠ D(Q||P)。特别的，在信息论中，D(P||Q)表示当用概率分布Q来拟合真实分布P时，产生的信息损耗，其中P表示真实分布，Q表示P的拟合分布。

![img](https://img-blog.csdn.net/20180623230535932?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzE0ODQ1MTE5/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

### 7.最大间隔损失large margin softmax loss:

​		用于拉大类间距离的损失函数，可以训练得到比传统softmax loss更好的分类效果。

![img](https://img-blog.csdn.net/20180623230733630?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzE0ODQ1MTE5/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

​		最大间隔损失主要引入了夹角cos值进行距离的度量。假设bias为0的情况下，就可以得出如上的公式。

​		其中fai(seita)需要满足下面的条件。

![img](https://img-blog.csdn.net/20180623231109895?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzE0ODQ1MTE5/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

​		为了进行距离的度量，在cos夹角中引入了参数m。该m为一个正整数，可以起到控制类间间隔的作用。M越大，类间间隔越大。当m=1时，等价于传统交叉熵损失。基本原理如下面公式

![img](https://img-blog.csdn.net/20180623231141294?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzE0ODQ1MTE5/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

论文中提供的满足该条件的公式如下

![img](https://img-blog.csdn.net/20180623231213814?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzE0ODQ1MTE5/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

### 8.中心损失center loss:

​		中心损失主要主要用于减少类内距离，虽然只是减少了类内距离，效果上却可以表现出类内距离小了，类间距离就可以增大的效果。该损失不可以直接使用，需要配合传统的softmax loss一起使用。可以起到比单纯softmax loss更好的分类效果。

![img](https://img-blog.csdn.net/20180623231316644?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzE0ODQ1MTE5/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

### 9.回归任务loss:

均方误差mean squareerror（MSE）和L2范数:

MSE表示了预测值与目标值之间差值的平方和然后求平均

![img](https://img-blog.csdn.net/20180623231552456?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzE0ODQ1MTE5/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

L2损失表示了预测值与目标值之间差值的平方和然后开更方，L2表示的是欧几里得距离。

![img](https://img-blog.csdn.net/20180623231614862?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzE0ODQ1MTE5/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

MSE和L2的曲线走势都一样。区别在于一个是求的平均np.mean()，一个是求的更方np.sqrt()

![img](https://img-blog.csdn.net/20180623231649937?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzE0ODQ1MTE5/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

### 10.平均绝对误差meanabsolute error(MAE )和L1范数:

MAE表示了预测值与目标值之间差值的绝对值然后求平均

![img](https://img-blog.csdn.net/20180623231910740?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzE0ODQ1MTE5/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)


L1表示了预测值与目标值之间差值的绝对值，L1也叫做曼哈顿距离

![img](https://img-blog.csdn.net/20180623231933282?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzE0ODQ1MTE5/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

MAE和L1的区别在于一个求了均值np.mean()，一个没有求np.sum()。2者的曲线走势也是完全一致的。

![img](https://img-blog.csdn.net/20180623232010305?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzE0ODQ1MTE5/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

MSE，MAE对比：

![img](https://img-blog.csdn.net/20180623232124252?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzE0ODQ1MTE5/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

MAE损失对于局外点更鲁棒，但它的导数不连续使得寻找最优解的过程低效；MSE损失对于局外点敏感，但在优化过程中更为稳定和准确。

### 11.Huber Loss和smooth L1：

Huber loss具备了MAE和MSE各自的优点，当δ趋向于0时它就退化成了MAE,而当δ趋向于无穷时则退化为了MSE。

![img](https://img-blog.csdn.net/20180623232247872?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzE0ODQ1MTE5/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)


Smooth L1 loss也具备了L1 loss和L2 loss各自的优点，本质就是L1和L2的组合。

![img](https://img-blog.csdn.net/2018062323230921?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzE0ODQ1MTE5/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

Huber loss和Smooth L1 loss具有相同的曲线走势，当Huber loss中的δ等于1时，Huber loss等价于Smooth L1 loss。

![img](https://img-blog.csdn.net/20180623232335403?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzE0ODQ1MTE5/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

对于Huber损失来说，δ的选择十分重要，它决定了模型处理局外点的行为。当残差大于δ时使用L1损失，很小时则使用更为合适的L2损失来进行优化。

Huber损失函数克服了MAE和MSE的缺点，不仅可以保持损失函数具有连续的导数，同时可以利用MSE梯度随误差减小的特性来得到更精确的最小值，也对局外点具有更好的鲁棒性。

但Huber损失函数的良好表现得益于精心训练的超参数δ。

对数双曲余弦logcosh:

![img](https://img-blog.csdn.net/20180623232520360?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzE0ODQ1MTE5/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)![img](https://img-blog.csdn.net/20180623232544999?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzE0ODQ1MTE5/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

​		其优点在于对于很小的误差来说log(cosh(x))与（x**2）/2很相近，而对于很大的误差则与abs(x)-log2很相近。这意味着logcosh损失函数可以在拥有MSE优点的同时也不会受到局外点的太多影响。它拥有Huber的所有优点，并且在每一个点都是二次可导的。



---------------------

## 二、平方损失函数（最小二乘法, Ordinary Least Squares ）

​		最小二乘法是线性回归的一种，最小二乘法（OLS）将问题转化成了一个凸优化问题。

​		在线性回归中，它假设样本和噪声都服从高斯分布（为什么假设成高斯分布呢？其实这里隐藏了一个小知识点，就是**中心极限定理**，可以参考[【central limit theorem】](https://en.wikipedia.org/wiki/Central_limit_theorem)），最后通过极大似然估计（MLE）可以推导出最小二乘式子。

​		最小二乘的基本原则是：**最优拟合直线应该是使各点到回归直线的距离和最小的直线，即平方和最小**。换言之，OLS是基于距离的，而这个距离就是我们用的最多的**欧几里得距离**。为什么它会选择使用欧式距离作为误差度量呢（即Mean squared error， MSE），主要有以下几个原因：

- 简单，计算方便；
- 欧氏距离是一种很好的相似性度量标准；
- 在不同的表示域变换后特征性质不变。

平方损失（Square loss）的标准形式如下：

![img](https://images2018.cnblogs.com/blog/1192699/201806/1192699-20180623134354357-118851936.png)

当样本个数为n时，此时的损失函数变为：

![img](https://images2018.cnblogs.com/blog/1192699/201806/1192699-20180623134429444-995232248.png)

`Y-f(X)`表示的是残差，整个式子表示的是**残差的平方和**，而我们的目的就是最小化这个目标函数值（注：该式子未加入正则项），也就是**最小化残差的平方和（residual sum of squares，RSS）**。

而在实际应用中，通常会使用均方差（MSE）作为一项衡量指标，公式如下：

![img](https://images2018.cnblogs.com/blog/1192699/201806/1192699-20180623134533400-1496909325.png)

上面提到了线性回归，这里额外补充一句，我们通常说的线性有两种情况，一种是因变量y是自变量x的线性函数，一种是因变量y是参数

## 三、指数损失函数（Adaboost）

​		学过Adaboost算法的人都知道，它是前向分步加法算法的特例，是一个加和模型，损失函数就是指数函数。在Adaboost中，经过m此迭代之后，可以得到fm(x):

![img](https://images2018.cnblogs.com/blog/1192699/201806/1192699-20180623134711775-2037666386.png)

Adaboost每次迭代时的目的是为了找到最小化下列式子时的参数α 和G：

![img](https://images2018.cnblogs.com/blog/1192699/201806/1192699-20180623134743521-1155320847.png)

**而指数损失函数（exp-loss）的标准形式如下**

![img](https://images2018.cnblogs.com/blog/1192699/201806/1192699-20180623134831896-110032820.png)

可以看出，Adaboost的目标式子就是指数损失，在给定n个样本的情况下，Adaboost的损失函数为：

![img](https://images2018.cnblogs.com/blog/1192699/201806/1192699-20180623134916410-2126629124.png)

关于Adaboost的推导，可以参考Wikipedia：[AdaBoost](https://en.wikipedia.org/wiki/AdaBoost)或者《统计学习方法》P145.

## 四、Hinge损失函数（SVM）

​		在机器学习算法中，hinge损失函数和SVM是息息相关的。在**线性支持向量机**中，最优化问题可以等价于下列式子：

![img](https://images2018.cnblogs.com/blog/1192699/201806/1192699-20180623141728667-1767421559.png)

下面来对式子做个变形，令：

![img](https://images2018.cnblogs.com/blog/1192699/201806/1192699-20180623142056432-1502037085.png)

于是，原式就变成了：

[![img](https://images2018.cnblogs.com/blog/1192699/201806/1192699-20180623142117904-1220113962.png)](http://latex.codecogs.com/gif.latex?%24%24\min_{w%2Cb} \ \sum_{i}^{N} \xi_i + \lambda||w||^2 %24%24)

如若取λ=1/(2C)，式子就可以表示成：

![img](https://images2018.cnblogs.com/blog/1192699/201806/1192699-20180623142157441-1125407759.png)

可以看出，该式子与下式非常相似：

![img](https://images2018.cnblogs.com/blog/1192699/201806/1192699-20180623142350407-235591645.png)

前半部分中的 l 就是hinge损失函数，而后面相当于L2正则项。

**Hinge 损失函数的标准形式**

![img](https://images2018.cnblogs.com/blog/1192699/201806/1192699-20180623142420807-325496303.png)

表示如果被正确分类，损失是0，否则损失就是
$$
1-m_i(w)
$$
可以看出，当|y|>=1时，L(y)=0。

在机器学习中，Hing 可以用来解 **间距最大化** 的问题，最有代表性的就是SVM 问题，最初的SVM 优化函数如下：
$$
\underset{w,\zeta}{argmin} \frac{1}{2}||w||^2+ C\sum_i \zeta_i \\
st.\quad \forall y_iw^Tx_i \geq 1- \zeta_i \\
\zeta_i \geq 0
$$
将约束项进行变形，则为： 
$$
\zeta_i \geq 1-y_iw^Tx_i
$$
 则损失函数可以进一步写为： 
$$
\begin{equation}\begin{split}J(w)&=\frac{1}{2}||w||^2 + C\sum_i max(0,1-y_iw^Tx_i) \\
&= \frac{1}{2}||w||^2 + C\sum_i max(0,1-m_i(w)) \\
&= \frac{1}{2}||w||^2 + C\sum_i L_{Hinge}(m_i)
\end{split}\end{equation}
$$
因此， **SVM 的损失函数可以看作是 L2-norm 和 Hinge loss 之和**。

更多内容，参考[Hinge-loss](https://en.wikipedia.org/wiki/Hinge_loss)。

补充一下：在libsvm中一共有4种核函数可以选择，对应的是`-t`参数分别是：

- 0-线性核；
- 1-多项式核；
- 2-RBF核；
- 3-sigmoid核。

## 五、其他损失函数

除了以上这几种损失函数，常用的还有：

**0-1损失函数**

![img](https://images2018.cnblogs.com/blog/1192699/201806/1192699-20180623142507596-329289242.png)

**绝对值损失函数**

![img](https://images2018.cnblogs.com/blog/1192699/201806/1192699-20180623142527347-1339325160.png)

下面来看看几种损失函数的可视化图像，对着图看看横坐标，看看纵坐标，再看看每条线都表示什么损失函数，多看几次好好消化消化。

![img](https://images2018.cnblogs.com/blog/1192699/201806/1192699-20180623142656630-1607226378.png)

## 六、Keras / TensorFlow 中常用 Cost Function 总结

- mean_squared_error或mse
- mean_absolute_error或mae
- mean_absolute_percentage_error或mape
- mean_squared_logarithmic_error或msle
- squared_hinge
- hinge
- categorical_hinge
- binary_crossentropy（亦称作对数损失，logloss）
- logcosh
- categorical_crossentropy：亦称作多类的对数损失，注意使用该目标函数时，需要将标签转化为形如`(nb_samples, nb_classes)`的二值序列
- sparse_categorical_crossentrop：如上，但接受稀疏标签。注意，使用该函数时仍然需要你的标签与输出值的维度相同，你可能需要在标签数据上增加一个维度：`np.expand_dims(y,-1)`
- kullback_leibler_divergence:从预测值概率分布Q到真值概率分布P的信息增益,用以度量两个分布的差异.
- poisson：即`(predictions - targets * log(predictions))`的均值
- cosine_proximity：即预测值与真实标签的余弦距离平均值的相反数

　　需要记住的是：**参数越多，模型越复杂，而越复杂的模型越容易过拟合**。过拟合就是说模型在训练数据上的效果远远好于在测试集上的性能。此时可以考虑正则化，通过设置正则项前面的hyper parameter，来权衡损失函数和正则项，减小参数规模，达到模型简化的目的，从而使模型具有更好的泛化能力。

## 七、Hinge loss 与 Softmax loss

SVM和Softmax分类器是最常用的两个分类器。

1. SVM将输出
   $$
   f(x_i,W)
   $$
   作为每个分类的评分(没有规定的标准,难以直接解释)；

2. 与SVM 不同，Softmax 分类器可以理解为逻辑回归分类器面对多个分类的一般话归纳，其输出(归一化的分类概率)更加直观,且可以从概率上解释。

   ​		在Softmax分类器中, 函数映射
   $$
   f(x_i,W)
   $$
   保持不变,但将这些评分值看做每个分类未归一化的对数概率,且将折叶损失替换为交叉熵损失(cross-entropy loss),公式如下:

$$
L_i= -\log(\frac{e^{f_{y_i}}}{\sum_je^{f_{j}}})
$$

或等价的 
$$
L_i= -f_{y_i} + \log \sum_j f_{j}
$$
fj 表示分类评分向量f中的第i 个元素,和SVM一样,整个数据集的损失值是数据集中所有样本数据的损失值Li的均值和正则化损失之和。

概率论解释: 
$$
P(y_i|x_i,W)= \frac{e^{f_{y_i}}}{\sum_je^{f_{j}}}
$$
解释为给定数据xi， WW参数,分配给正确分类标签yi的归一化概率。		![1562948429293](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\1562948429293.png)

该技巧就是将向量f中的数值进行平移,使得最大值为0。

​		准确地说，SVM分类器使用的是铰链损失（hinge loss），有时候又被称为最大边界损失（max-margin loss）。Softmax分类器使用的是交叉熵损失（corss-entropy loss）。Softmax分类器的命名是从softmax函数那里得来的，softmax函数将原始分类评分变成正的归一化数值，所有数值和为1，这样处理后交叉熵损失才能应用。



​		针对给出的图像，SVM分类器可能给你的是一个\[−2.85,0.86,0.28][−2.85,0.86,0.28] 对应分类“猫”，“狗”，“船”，而softmax分类器可以计算出这三个标签的”可能性“是\[0.,0160.631,0.353][0.,0160.631,0.353] ，这就让你能看出对于不同分类准确性的把握。

这里Hinge Loss计算公式为： 
$$
L_i = \sum_{j \neq y_i} \max (0,f(x_i,W)_j - f(x_i,W))_{y_i} + \Delta
$$
这里 Δ 是一个阈值，表示即使误分类，但是没有达到阈值，也不存在损失 。上面的公式把错误类别 (j≠yi)(j≠yi) 都遍历一遍，求值加和。

设 xi 的正确类别是”船”，阈值 Δ=1 ，则对应的Hinge loss 为： 
$$
L_i = \max(0,-2.85-0.28+1) + \max(0,0.86-0.28+1) = 1.58
$$
下图是对ΔΔ的理解，蓝色表示正确的类别，Δ 表示一个安全范围，就算是有其他的得分，只要没有到达红色的ΔΔ范围内,，对损失函数都没有影响。这就保证了SVM 算法的解的稀疏性。


而Softmax 损失则是对向量 fyi指数正规化得到概率，再求对数即可。 
$$
L_i= -\log(\frac{e^{f_{y_i}}}{\sum_je^{f_{j}}}) = -\log(0.353)  \approx 1.04
$$

## 八、总结

各损失函数图形如下：

![loss_function](https://img-blog.csdn.net/20160907191425949)

​		机器学习作为一种优化方法，学习目标就是找到优化的目标函数——损失函数和正则项的组合；有了目标函数的“正确的打开方式”，才能通过合适的机器学习算法求解优化。

​		不同机器学习方法的损失函数有差异，合理理解各种损失优化函数的的特点更有利于我们对相关算法的理解。