[TOC]

# 分类和回归

## 1. 概念

​		分类模型和回归模型本质一样，分类模型将回归模型的输出离散化，回归模型可将分类模型的输出连续化。

## 2. 例子

1. Logistic Regression和Linear Regression:

   1. Linear Regression:输出一个标量wx+b,这个值是连续值,所以可以用来处理回归问题
   2. Logistic Regression:把上面的wx+b通过sigmoid函数映射到0,1)上,并划分一个阈值,大于阈值的分为一类,小于等于分为另一类,可以用来处理二分类问题 
   3. 更进一步:对于N分类问题,则是先得到N组w值不同的wx+b,然后归一化,比如用 softmax函数,最后变成N个类上的概率,可以处理多分类问题

2. Support Vector Regression和Support Vector Machine:

   1. SVR:输出wx+b,即某个样本点到分类面的距离,是连续值,所以是回归模型

   2. SVM:把这个距离用sign()函数作用,距离为正(在超平面一侧的样本点是一类,为负的是另一类,所以是分类模型

3. Naive Bayes用于分类和回归:

   1. 用于分类: y是离散的类别,所以得到离散的p(ylx),给定x,输出每个类上的概率

   2. 用于回归:对上面离散的p(yx)求期望zyP(ylx),就得到连续值。但因为此时y本身是连续的直,所以最地道的做法是,得到连续的概率密度函数p(ylx),然后再对y求期望。参考cs.waikato.ac.nz/~eibe/...

4. 前馈神经网络(如CNN系列)用于分类和回归:

   1. 用于回归:最后一层有m个神经元,每个神经元输出一个标量, m个神经元的输出可以看做向量v,现全部连到一个神经元上,则这个神经元输出wv+b,是一个连续值,可以处理回归问题,跟上面Linear Regression思想一样

   2. 用于N分类:现在这m个神经元最后连接到N个神经元,就有N组w值不同的wv+b,同理可以归一化(比如用softmax )变成N个类上的概率(补充一下,如果不用softmax,而是每个wx+b用一个sigmoid,就变成多标签问题,跟多分类的区别在于,样本可以被打上多个标签)

5. 循环神经网络(如RNN系列)用于分类和回归: 

   1. 用于回归和分类:跟CNN类似,输出层的值y=w+b,可做分类可做回归,只不过区别在于, RNN的输出跟时间有关,即输出的是(y(t) y(t+ 1)..列(关于时间序列,见下面的更新)

