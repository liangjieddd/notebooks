[TOC]

# 支持向量机通俗导论

## 1.前言

​		支持向量机，因其英文名为support vector machine，故一般简称SVM，通俗来讲，它是一种**二类分类模型**，其基本模型定义为**特征空间上的间隔最大的线性分类器**，其学习策略便是**间隔最大化**，最终可转化为一个**凸二次规划问题**的求解。

## 2.分类标准的起源：Logistic回归

### 2.1 简介svm

​		理解SVM，咱们必须先弄清楚一个概念：**线性分类器**。

​		给定一些数据点，它们分别属于**两个不同的类**，现在要找到一个**线性分类器**把这些数据分成两类。

​		如果用x表示数据点，用y表示类别（y可以取1或者-1，分别代表两个不同的类），一个线性分类器的学习目标便是要在n维的数据空间中找到一个超平面（hyper plane），这个超平面的方程可以表示为（ wT中的T代表转置）：

![img](https://img-blog.csdn.net/20131107201104906)

​		可能有读者对类别取1或-1有疑问，事实上，这个1或-1的分类标准起源于logistic回归。

### 2.2 Logistic 回归

​		Logistic回归目的是**从特征学习出一个0/1分类模型**，而这个模型是将特性的线性组合作为自变量，由于自变量的取值范围是负无穷到正无穷。因此，使用**logistic函数（或称作sigmoid函数）将自变量映射到(0,1)上**，映射后的值被认为是属于y=1的概率。

​		假设函数

![img](https://img-my.csdn.net/uploads/201304/05/1365174192_8325.png)

其中x是n维特征向量，函数g就是logistic函数。

而![img](https://img-my.csdn.net/uploads/201304/05/1365174223_1807.png)的图像是

![img](https://img-my.csdn.net/uploads/201304/05/1365174236_6175.png)

可以看到，将无穷映射到了(0,1)。

而假设函数就是特征属于y=1的概率。

![img](https://img-my.csdn.net/uploads/201304/05/1365174921_9452.png)

​		从而，当我们要判别一个新来的特征属于哪个类时，只需求![img](https://img-my.csdn.net/uploads/201304/05/1365175136_8232.png)即可，若![img](https://img-my.csdn.net/uploads/201304/05/1365175136_8232.png)大于0.5就是y=1的类，反之属于y=0类。

​		此外，![img](https://img-my.csdn.net/uploads/201304/05/1365175136_8232.png)只和![img](https://img-my.csdn.net/uploads/201304/05/1365175161_1760.png)有关，![img](https://img-my.csdn.net/uploads/201304/05/1365175169_2349.png)>0，那么![img](https://img-my.csdn.net/uploads/201304/05/1365175178_8905.png)，而g(z)只是用来映射，真实的类别决定权还是在于![img](https://img-my.csdn.net/uploads/201304/05/1365175189_9269.png)。

​		再者，当![img](https://img-my.csdn.net/uploads/201304/05/1365175205_2324.png)时，![img](https://img-my.csdn.net/uploads/201304/05/1365175215_8446.png)=1，反之![img](https://img-my.csdn.net/uploads/201304/05/1365175215_8446.png)=0。如果我们只从![img](https://img-my.csdn.net/uploads/201304/05/1365175266_3733.png)出发，希望模型达到的目标就是让训练数据中y=1的特征![img](https://img-my.csdn.net/uploads/201304/05/1365175288_2654.png)，而是y=0的特征![img](https://img-my.csdn.net/uploads/201304/05/1365175299_9597.png)。

​		Logistic回归就是要学习得到![img](https://img-my.csdn.net/uploads/201304/05/1365175329_6408.png)，使得正例的特征远大于0，负例的特征远小于0，而且要在全部训练实例上达到这个目标。

接下来，尝试把logistic回归做个变形。首先，将使用的结果标签y = 0和y = 1替换为y = -1,y = 1，然后将![img](https://img-my.csdn.net/uploads/201304/05/1365175711_9116.png)（![img](https://img-my.csdn.net/uploads/201304/05/1365175723_3132.png)）中的![img](https://img-blog.csdn.net/20140826150648949)替换为b，最后将后面的![img](https://img-my.csdn.net/uploads/201304/05/1365175737_9557.png)替换为wTx（即![img](https://img-my.csdn.net/uploads/201304/05/1365175756_2693.png)）。如此，则有了![img](https://img-my.csdn.net/uploads/201304/05/1365175767_8636.png)。也就是说除了y由y=0变为y=-1外，线性分类函数跟logistic回归的形式化表示![img](https://img-my.csdn.net/uploads/201304/05/1365175792_4997.png)没区别。

进一步，可以将假设函数![img](https://img-my.csdn.net/uploads/201304/05/1365175830_5193.png)中的g(z)做一个简化，将其简单映射到y=-1和y=1上。映射关系如下：

​      ![img](https://img-my.csdn.net/uploads/201304/05/1365175998_9759.png)            

## 3.线性分类的例子

​		下面举个简单的例子。如下图所示，现在有一个二维平面，平面上有两种不同的数据，分别用圈和叉表示。由于这些数据是线性可分的，所以可以用一条直线将这两类数据分开，这条直线就相当于一个超平面，超平面一边的数据点所对应的y全是-1 ，另一边所对应的y全是1。

![img](https://img-blog.csdn.net/20140829134124453)

​		这个超平面可以用分类函数![img](https://img-blog.csdn.net/20131107201211968)表示，当f(x) 等于0的时候，x便是位于超平面上的点，而f(x)大于0的点对应 y=1 的数据点，f(x)小于0的点对应y=-1的点，如下图所示：

![img](https://img-blog.csdn.net/20140829134548371)

 ![1563534494300](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\1563534494300.png)

## 4.函数间隔Functional margin与几何间隔Geometrical margin

​		在超平面w*x+b=0确定的情况下，|w*x+b|能够表示点x到距离超平面的远近，而通过观察w*x+b的符号与类标记y的符号是否一致可判断分类是否正确，所以，可以用(y*(w*x+b))的正负性来判定或表示分类的正确性。于此，我们便引出了函数间隔（functional margin）的概念。

​		定义函数间隔（用![img](https://img-blog.csdn.net/20140829135049264)表示）为：

![img](https://img-blog.csdn.net/20131107201248921)

​		而超平面(w，b)关于T中所有样本点(xi，yi)的函数间隔最小值（其中，x是特征，y是结果标签，i表示第i个样本），便为超平面(w, b)关于训练数据集T的函数间隔：

 ![img](https://img-blog.csdn.net/20131111154113734)= min![img](https://img-blog.csdn.net/20131111154113734)i  (i=1，...n)

​		但这样定义的函数间隔有问题，即如果成比例的改变w和b（如将它们改成2w和2b），则函数间隔的值f(x)却变成了原来的2倍（虽然此时超平面没有改变），所以只有函数间隔还远远不够。

事实上，我们可以对法向量w加些约束条件，从而引出真正定义点到超平面的距离--几何间隔（geometrical margin）的概念。

假定对于一个点 x ，令其垂直投影到超平面上的对应点为 x0 ，w 是垂直于超平面的一个向量，为样本x到超平面的距离，如下图所示：

 ![img](http://blog.pluskid.org/wp-content/uploads/2010/09/geometric_margin.png)

根据平面几何知识，有

![img](https://img-blog.csdn.net/20131107201720515)

​		其中||w||为w的二阶范数（范数是一个类似于模的表示长度的概念），![img](https://img-blog.csdn.net/20160118171730323)是单位向量（一个向量除以它的模称之为单位向量）。

​		又由于*x*0 是超平面上的点，满足 *f*(*x*0)=0，代入超平面的方程![img](https://img-blog.csdn.net/20131107201104906)，可得![img](https://img-blog.csdn.net/20160117004048499)，即![img](https://img-blog.csdn.net/20160117004117802)。

​		随即让此式![img](https://img-blog.csdn.net/20131107201720515)的两边同时乘以![img](https://img-blog.csdn.net/20160117004629562)，再根据![img](https://img-blog.csdn.net/20160117004117802)和![img](https://img-blog.csdn.net/20160117004138049)，即可算出*γ*： 

![img](https://img-blog.csdn.net/20131107201759093)

​		为了得到![img](https://img-blog.csdn.net/20140829135315499)的绝对值，令![img](https://img-blog.csdn.net/20140829135315499)乘上对应的类别 y，即可得出几何间隔（用![img](https://img-blog.csdn.net/20140829135609579)表示）的定义：

![img](https://img-blog.csdn.net/20131107201919484)

​		从上述函数间隔和几何间隔的定义可以看出：**几何间隔就是函数间隔除以||w||**，而且函数间隔y*(wx+b) = y*f(x)实际上就是|f(x)|，只是人为定义的一个间隔度量，而**几何间隔|f(x)|/||w||才是直观上的点到超平面的距离。**

## 5.最大间隔分类器Maximum Margin Classifier的定义

​		对一个数据点进行分类，当超平面离数据点的“间隔”越大，分类的确信度（confidence）也越大。所以，为了使得分类的确信度尽量高，需要让所选择的超平面能够最大化这个“间隔”值。这个间隔就是下图中的Gap的一半。

![img](https://img-blog.csdn.net/20140829135959290)



​		通过由前面的分析可知：**函数间隔不适合用来最大化间隔值**，因为在超平面固定以后，可以等比例地缩放w的长度和b的值，这样可以使得![img](https://img-blog.csdn.net/20131107201211968)的值任意大，亦即函数间隔![img](https://img-blog.csdn.net/20131111154113734)可以在超平面保持不变的情况下被取得任意大。

​		但几何间隔因为除上了![img](https://img-blog.csdn.net/20131111154326078)，使得在缩放w和b的时候几何间隔![img](https://img-blog.csdn.net/20131111154113734)的值是不会改变的，它只随着超平面的变动而变动，因此，这是更加合适的一个间隔。换言之，这里要找的最大间隔分类超平面中的“间隔”指的是**几何间隔**。

​		于是最大间隔分类器（maximum margin classifier）的目标函数可以定义为：

![img](https://img-blog.csdn.net/20131111160612687)

​		同时需满足一些条件，根据间隔的定义，有

![img](https://img-my.csdn.net/uploads/201210/25/1351141813_4166.jpg)

​		其中，s.t.，即subject to的意思，它导出的是约束条件。

​		回顾下几何间隔的定义![img](https://img-blog.csdn.net/20131107201919484)，可知：如果令函数间隔![img](https://img-blog.csdn.net/20131111154113734)等于1（之所以令![img](https://img-blog.csdn.net/20131111154113734)等于1，是为了方便推导和优化，且这样做对目标函数的优化没有影响，至于为什么，请见本文评论下第42楼回复），则有![img](https://img-blog.csdn.net/20131111154137453) = 1 / ||w||且![img](https://img-blog.csdn.net/20140829140642940)，从而上述目标函数转化成了

![img](https://img-my.csdn.net/uploads/201210/25/1351141837_7366.jpg)

​		相当于在相应的约束条件![img](https://img-blog.csdn.net/20140829140642940)下，最大化这个1/||w||值，而1/||w||便是几何间隔![img](https://img-blog.csdn.net/20131111154137453)。   

​		如下图所示，中间的实线便是寻找到的最优超平面（Optimal Hyper Plane），其到两条虚线边界的距离相等，这个距离便是几何间隔![img](https://img-blog.csdn.net/20131111154137453)，两条虚线间隔边界之间的距离等于2![img](https://img-blog.csdn.net/20131111154137453)，而虚线间隔边界上的点则是支持向量。

​		由于这些支持向量刚好在虚线间隔边界上，所以它们满足![img](https://img-blog.csdn.net/20131111155244218)（还记得我们把 functional margin 定为 1 了吗？上节中：处于方便推导和优化的目的，我们可以令![img](https://img-blog.csdn.net/20131111154137453)=1），而对于所有不是支持向量的点，则显然有![img](https://img-blog.csdn.net/20131111155205109)。

![img](https://img-blog.csdn.net/20140829141714944)

​		OK，到此为止，算是了解到了SVM的第一层，对于那些只关心怎么用SVM的朋友便已足够，不必再更进一层深究其更深的原理。

## 6. 从线性可分到线性不可分

### 6.1 **从原始问题到对偶问题的求解**

​		接着考虑之前得到的目标函数：

![img](https://img-my.csdn.net/uploads/201210/25/1351141975_6347.jpg)

​     		由于求![img](https://img-my.csdn.net/uploads/201301/11/1357837136_7540.png)的最大值相当于求![img](https://img-my.csdn.net/uploads/201301/11/1357837152_4634.png)的最小值，所以上述目标函数等价于（w由分母变成分子，从而也有原来的max问题变为min问题，很明显，两者问题等价）：

![img](https://img-my.csdn.net/uploads/201210/25/1351141994_1802.jpg)

​		因为现在的目标函数是二次的，约束条件是线性的，所以它是一个凸二次规划问题。这个问题可以用现成的[QP (Quadratic Programming)](http://en.wikipedia.org/wiki/Quadratic_programming) 优化包进行求解。一言以蔽之：**在一定的约束条件下，目标最优，损失最小。**

​		此外，由于这个问题的特殊结构，还可以通过拉格朗日对偶性（Lagrange Duality）变换到对偶变量 (dual variable) 的优化问题，即通过求解与原问题等价的对偶问题（dual problem）得到原始问题的最优解，这就是线性可分条件下支持向量机的对偶算法，这样做的优点在于：一者对偶问题往往更容易求解；二者可以自然的引入核函数，进而推广到非线性分类问题。

​		那什么是拉格朗日对偶性呢？简单来讲，通过给每一个约束条件加上一个**拉格朗日乘子（Lagrange multiplier）![img](https://img-blog.csdn.net/20131111195836468)**，定义拉格朗日函数（通过拉格朗日函数将约束条件融合到目标函数里去，从而只用一个函数表达式便能清楚的表达出我们的问题）：·

![img](https://img-my.csdn.net/uploads/201210/25/1351142114_6643.jpg)

然后令

![img](https://img-my.csdn.net/uploads/201210/25/1351142171_6289.jpg)

​		容易验证，当某个约束条件不满足时，例如![img](https://img-blog.csdn.net/20131107202615937)，那么显然有![img](https://img-blog.csdn.net/20131107202642843)（只要令![img](https://img-blog.csdn.net/20131107202702265)即可）。而当所有约束条件都满足时，则最优值为![img](https://img-blog.csdn.net/20131111195433031)，亦即最初要最小化的量。

​		因此，在要求约束条件得到满足的情况下最小化![img](https://img-blog.csdn.net/20131111195324546)，实际上等价于直接最小化![img](https://img-blog.csdn.net/20131111195552578)（当然，这里也有约束条件，就是![img](https://img-blog.csdn.net/20131111195824031)≥0,*i*=1,…,*n*）   ，因为如果约束条件没有得到满足，![img](https://img-blog.csdn.net/20131111195552578)会等于无穷大，自然不会是我们所要求的最小值。

​    具体写出来，目标函数变成了：

![img](https://img-my.csdn.net/uploads/201210/25/1351142295_1902.jpg)

这里用![img](https://img-blog.csdn.net/20131107202721703)表示这个问题的最优值，且和最初的问题是等价的。如果直接求解，那么一上来便得面对w和b两个参数，而![img](https://img-blog.csdn.net/20131111195824031)又是不等式约束，这个求解过程不好做。不妨把最小和最大的位置交换一下，变成：

![img](https://img-my.csdn.net/uploads/201210/25/1351142316_5141.jpg)

交换以后的新问题是原始问题的对偶问题，这个新问题的最优值用![img](https://img-blog.csdn.net/20131107202736187)来表示。而且有![img](https://img-blog.csdn.net/20131107202736187)≤![img](https://img-blog.csdn.net/20131107202721703)，在满足某些条件的情况下，这两者相等，这个时候就可以通过求解对偶问题来间接地求解原始问题。

换言之，之所以从minmax的原始问题![img](https://img-blog.csdn.net/20131107202721703)，转化为maxmin的对偶问题![img](https://img-blog.csdn.net/20131107202736187)，一者因为![img](https://img-blog.csdn.net/20131107202736187)是![img](https://img-blog.csdn.net/20131107202721703)的近似解，二者，转化为对偶问题后，更容易求解。

下面可以先求L 对w、b的极小，再求L 对![img](https://img-blog.csdn.net/20131111195836468)的极大。

### 6.2 KKT条件

​		上文中提到“![img](https://img-blog.csdn.net/20131107202736187)≤![img](https://img-blog.csdn.net/20131107202721703)在满足某些条件的情况下，两者等价”，这所谓的“满足某些条件”就是要满足KKT条件。

​		勘误：经读者qq_28543029指出，这里的条件不应该是KKT条件，要让两者等价需满足strong duality （强对偶），而后有学者在强对偶下提出了KKT条件，且KKT条件的成立要满足constraint qualifications，而constraint qualifications之一就是Slater条件。所谓Slater 条件，即指：**凸优化问题，如果存在一个点x，使得所有等式约束都成立，并且所有不等式约束都严格成立（即取严格不等号，而非等号），则满足Slater 条件。**对于此处，Slater 条件成立，所以d*≤p*可以取等号。

一般地，一个最优化数学模型能够表示成下列标准形式：

![img](https://img-blog.csdnimg.cn/20190127114042574.jpg)

​		其中，f(x)是需要最小化的函数，h(x)是等式约束，g(x)是不等式约束，p和q分别为等式约束和不等式约束的数量。

​    同时，得明白以下两点：

- 凸优化的概念：![\mathcal{X} \subset \mathbb{R}^n](http://upload.wikimedia.org/math/d/0/1/d01e9255365440ae709190fafc071951.png) 为一凸集， ![f:\mathcal{X}\to \mathbb{R}](http://upload.wikimedia.org/math/3/4/5/345879b44bce56b80552389916fa67fe.png) 为一凸函数。凸优化就是要找出一点 ![x^\ast \in \mathcal{X}](http://upload.wikimedia.org/math/7/a/b/7ab2b524ce2a695903b81d45d27d5242.png) ，使得每一 ![x \in \mathcal{X}](http://upload.wikimedia.org/math/6/2/4/624cf12f420fb0f373cda9f7b216b2f3.png) 满足 ![f(x^\ast)\le f(x)](http://upload.wikimedia.org/math/d/a/0/da0d27822f8c98efc3d1a39ae37f30e1.png) 。
- KKT条件的意义：它是一个非线性规划（Nonlinear Programming）问题能有最优化解法的必要和充分条件。

​        而KKT条件就是指上面最优化数学模型的标准形式中的最小点 x* 必须满足下面的条件：

![img](https://img-blog.csdnimg.cn/20190127114104532.jpg)

​		经过论证，我们这里的问题是满足 KKT 条件的（首先已经满足Slater条件，再者f和gi也都是可微的，即L对w和b都可导），因此现在我们便转化为求解第二个问题。

​		也就是说，原始问题通过满足KKT条件，已经转化成了对偶问题。而求解这个对偶学习问题，分为3个步骤：首先要让L(w，b，a) 关于 w 和 b 最小化，然后求对![img](https://img-blog.csdn.net/20131111195836468)的极大，最后利用SMO算法求解对偶问题中的拉格朗日乘子

### 6.3 **对偶问题求解的3个步骤**

**（1）**、首先固定*![img](https://img-blog.csdn.net/20131111195836468)，*要让 L 关于 w 和 b 最小化，我们分别对w，b求偏导数，即令 ∂L/∂w 和 ∂L/∂b 等于零（对w求导结果的解释请看本文评论下第45楼回复）：

![img](https://img-blog.csdn.net/20131107202220500)

将以上结果代入之前的L： 

![img](https://img-my.csdn.net/uploads/201210/25/1351142114_6643.jpg)

得到：

![img](https://img-my.csdn.net/uploads/201210/25/1351142449_6864.jpg)

提醒：有读者可能会问上述推导过程如何而来？说实话，其具体推导过程是比较复杂的，如下图所示：

![img](https://img-my.csdn.net/uploads/201301/11/1357837605_5830.png)

  最后，得到：

![img](https://img-my.csdn.net/uploads/201210/25/1351142449_6864.jpg)

​		如 jerrylead所说：“倒数第4步”推导到“倒数第3步”使用了线性代数的转置运算，由于ai和yi都是实数，因此转置后与自身一样。

​		“倒数第3步”推导到“倒数第2步”使用了(a+b+c+…)(a+b+c+…)=aa+ab+ac+ba+bb+bc+…的乘法运算法则。最后一步是上一步的顺序调整。

​		从上面的最后一个式子，我们可以看出，此时的拉格朗日函数只包含了一个变量，那就是![img](https://img-blog.csdn.net/20131111195824031)（求出了![img](https://img-blog.csdn.net/20131111195824031)便能求出w，和b，由此可见，上文第1.2节提出来的核心问题：分类函数**![img](https://img-blog.csdn.net/20131107201211968)**也就可以轻而易举的求出来了）。

**（2）**、求对*![img](https://img-blog.csdn.net/20131111195836468)的极大，*即是关于对偶问题的最优化问题。经过上面第一个步骤的求w和b，得到的拉格朗日函数式子已经没有了变量w，b，只有![img](https://img-blog.csdn.net/20131111195836468)。从上面的式子得到：

![img](https://img-blog.csdnimg.cn/20190127114132410.jpg)

  		这样，求出了![img](https://img-blog.csdn.net/20131111195824031)，根据![img](https://img-my.csdn.net/uploads/201301/11/1357838666_9138.jpg)，即可求出w，然后通过![img](https://img-my.csdn.net/uploads/201301/11/1357838696_3314.png)，即可求出b，最终得出分离超平面和分类决策函数。

**（3）**在求得L(w, b, a) 关于 w 和 b 最小化，以及对![img](https://img-blog.csdn.net/20131111195836468)的极大之后，最后一步则可以利用SMO算法求解对偶问题中的拉格朗日乘子![img](https://img-blog.csdn.net/20131111195836468)。

![img](https://img-blog.csdnimg.cn/20190127114239385.jpg)

​		上述式子要解决的是在参数![img](https://img-my.csdn.net/uploads/201304/05/1365176671_1627.png)上求最大值W的问题，至于![img](https://img-my.csdn.net/uploads/201304/05/1365176682_4857.png)和![img](https://img-my.csdn.net/uploads/201304/05/1365176690_4143.png)都是已知数。要了解这个SMO算法是如何推导的，请跳到下文第3.5节、SMO算法。

​		到目前为止，我们的 SVM 还比较弱，只能处理线性的情况，下面我们将引入核函数，进而推广到非线性分类问题。

### 6.4 **线性不可分的情况**

​		OK，为过渡到下节2.2节所介绍的核函数，让我们再来看看上述推导过程中得到的一些有趣的形式。首先就是关于我们的 hyper plane ，对于一个数据点 x 进行分类，实际上是通过把 x 带入到![img](https://img-blog.csdn.net/20131107201211968)算出结果然后根据其正负号来进行类别划分的。而前面的推导中我们得到 ![img](https://img-blog.csdn.net/20131111163543781)

​		因此**分类函数**为：

![img](https://img-my.csdn.net/uploads/201210/25/1351142572_5782.jpg)

​		这里的形式的有趣之处在于，**对于新点 x的预测，只需要计算它与训练数据点的内积即可**（![img](https://img-blog.csdn.net/20131111163753093)表示向量内积），这一点至关重要，是之后使用 Kernel 进行非线性推广的基本前提。

​		此外，所谓 Supporting Vector 也在这里显示出来——事实上，**所有非Supporting Vector 所对应的系数都是等于零的**，因此对于新点的内积计算实际上只要针对少量的“支持向量”而不是所有的训练数据即可。

​		为什么非支持向量对应的![img](https://img-blog.csdn.net/20131111164022593)等于零呢？直观上来理解的话，就是这些“后方”的点——正如我们之前分析过的一样，对超平面是没有影响的，由于分类完全有超平面决定，所以这些无关的点并不会参与分类问题的计算，因而也就不会产生任何影响了。

​		回忆一下我们2.1.1节中通过 Lagrange multiplier得到的目标函数：

![img](https://img-my.csdn.net/uploads/201210/25/1351142613_4680.jpg)	

​		注意到如果 xi 是支持向量的话，上式中红颜色的部分是等于 0 的（因为支持向量的 functional margin 等于 1 ），而对于非支持向量来说，functional margin 会大于 1 ，因此红颜色部分是大于零的，而又是非负的，为了满足最大化，![img](https://img-blog.csdn.net/20131111163835968)必须等于 0 。这也就是这些非Supporting Vector 的点的局限性。

​		至此，我们便得到了一个maximum margin hyper plane classifier，这就是所谓的支持向量机（Support Vector Machine）。当然，到目前为止，我们的 SVM 还比较弱，只能处理线性的情况，不过，在得到了对偶dual 形式之后，通过 **Kernel 推广到非线性**的情况就变成了一件非常容易的事情了(相信，你还记得本节开头所说的：“通过求解对偶问题得到最优解，这就是线性可分条件下支持向量机的对偶算法，这样做的优点在于：一者对偶问题往往更容易求解；二者可以自然的引入核函数，进而推广到非线性分类问题”)。

## 7.核函数Kernel

### 7.1 **特征空间的隐式映射：核函数**

​		事实上，大部分时候数据并不是线性可分的，这个时候满足这样条件的超平面就根本不存在。在上文中，我们已经了解到了SVM处理线性可分的情况，那对于非线性的数据SVM咋处理呢？

​		对于非线性的情况，SVM 的处理方法是选择一个**核函数 κ(⋅,⋅)** ，通过将数据**映射到高维空间**，来解决在原始空间中线性不可分的问题。

​		具体来说，在线性不可分的情况下，支持向量机首先在低维空间中完成计算，然后**通过核函数将输入空间映射到高维特征空间**，最终在高维特征空间中构造出**最优分离超平面**，从而把平面上本身不好分的非线性数据分开。如图所示，一堆数据在二维空间无法划分，从而映射到三维空间里划分：

![img](https://img-blog.csdn.net/20140830002108254)

​		而在我们遇到核函数之前，如果用原始的方法，那么在用线性学习器学习一个非线性关系，需要选择一个非线性特征集，并且将数据写成新的表达形式，这等价于应用一个固定的非线性映射，将数据映射到特征空间，在特征空间中使用线性学习器，因此，考虑的假设集是这种类型的函数：

![img](https://img-blog.csdnimg.cn/20190127114318959.JPG)

这里*ϕ*：X->F是从输入空间到某个特征空间的映射，这意味着建立非线性学习器分为两步：

1. 首先使用一个非线性映射将数据变换到一个特征空间F，
2. 然后在特征空间使用线性学习器分类。

​		而由于对偶形式就是线性学习器的一个重要性质，这意味着假设可以表达为训练点的线性组合，因此决策规则可以用测试点和训练点的内积来表示：

![img](https://img-blog.csdnimg.cn/20190127114343476.JPG)

​		如果有一种方式可以**在特征空间中直接计算内积〈φ(xi · φ(x)〉**，就像在原始输入点的函数中一样，就有可能将两个步骤融合到一起建立一个非线性的学习器，**这样直接计算法的方法称为核函数方法：**

​		核是一个函数K，对所有x，z(-X，满足![img](https://img-my.csdn.net/uploads/201206/04/1338741445_1451.JPG)![img](https://img-blog.csdnimg.cn/20190127114355685.JPG)，这里φ是从X到内积特征空间F的映射。

### 7.2**核函数：如何处理非线性数据**

​		来看个核函数的例子。如下图所示的两类数据，分别分布为两个圆圈的形状，这样的数据本身就是线性不可分的，此时咱们该如何把这两类数据分开呢(下文将会有一个相应的三维空间图)？

​		![img](https://img-blog.csdnimg.cn/2019012711441777.png)

​		事实上，上图所述的这个数据集，是用两个半径不同的圆圈加上了少量的噪音生成得到的，所以，一个理想的分界应该是一个“圆圈”而不是一条线（超平面）。

​		如果用![img](https://img-blog.csdn.net/20180516164047334)和![img](https://img-blog.csdn.net/20180516000114369)来表示这个二维平面的两个坐标的话，我们知道一条二次曲线（圆圈是二次曲线的一种特殊情况）的方程可以写作这样的形式：

![img](https://img-blog.csdn.net/20130820145508875)

​		注意上面的形式，如果我们构造另外一个五维的空间，其中五个坐标的值分别为![img](https://img-blog.csdn.net/20180509172308957), ![img](https://img-blog.csdn.net/20180509172319524), ![img](https://img-blog.csdn.net/20180509172331439), ![img](https://img-blog.csdn.net/20180509172339511)，![img](https://img-blog.csdn.net/20180509172349341)，那么显然，上面的方程在新的坐标系下可以写作：

![img](https://img-blog.csdn.net/20130820145522437)

​		关于新的坐标![img](https://img-blog.csdn.net/20180515235937497)，这正是一个 hyper plane 的方程！也就是说，如果我们做一个映射![img](https://img-blog.csdn.net/20180515235946910)，将 ![img](https://img-blog.csdn.net/20180515235953955)按照上面的规则映射为![img](https://img-blog.csdn.net/20180515235937497)，那么在新的空间中原来的数据将变成线性可分的，从而使用之前我们推导的线性分类算法就可以进行处理了。这正是 Kernel 方法处理非线性问题的基本思想。

​		再进一步描述 Kernel 的细节之前，不妨再来看看上述例子在映射过后的直观形态。当然，你我可能无法把 5 维空间画出来，不过由于我这里生成数据的时候用了特殊的情形，所以这里的超平面实际的方程是这个样子的（圆心在![img](https://img-blog.csdn.net/20180516000114369)轴上的一个正圆）：

![img](https://img-blog.csdn.net/20130820145544562)

​		因此我只需要把它映射到![img](https://img-blog.csdn.net/20180516000827595)，![img](https://img-blog.csdn.net/20180516000840802)，![img](https://img-blog.csdn.net/20180516000847353)这样一个三维空间中即可，下图即是映射之后的结果，将坐标轴经过适当的旋转，就可以很明显地看出，数据是可以通过一个平面来分开的(pluskid：下面的gif 动画，先用 Matlab 画出一张张图片，再用 Imagemagick 拼贴成)：

![img](https://img-my.csdn.net/uploads/201304/03/1364952814_3505.gif)

核函数相当于把原来的分类函数：

![img](https://img-my.csdn.net/uploads/201210/25/1351142877_8481.jpg)

映射成：

![img](https://img-my.csdn.net/uploads/201210/25/1351142890_4908.jpg)

 而其中的![img](https://img-blog.csdn.net/20131111164022593)可以通过求解如下 dual 问题而得到的：

![img](https://img-my.csdn.net/uploads/201210/25/1351142906_9411.jpg)

​		这样一来问题就解决了吗？似乎是的：拿到非线性数据，就找一个映射![img](https://img-my.csdn.net/uploads/201304/03/1364953575_7320.jpg)，然后一股脑把原来的数据映射到新空间中，再做线性 SVM 即可。不过事实上好像并没有这么简单。

细想一下，刚才的方法是不是有问题？

- 在最初的例子里，我们对一个二维空间做映射，选择的新空间是原始空间的所有一阶和二阶的组合，得到了五个维度；
- 如果原始空间是三维（一阶、二阶和三阶的组合），那么我们会得到：3(一次)+3(二次交叉)+3(平方)+3(立方)+1(x1\*x2\*x3)+2\*3(交叉，一个一次一个二次，类似x1*x2^2) = 19维的新空间，这个数目是呈指数级爆炸性增长的，从而势必这给的计算带来非常大的困难，而且如果遇到无穷维的情况，就根本无从计算了。

这个时候，可能就需要 Kernel 出马了。

​		不妨还是从最开始的简单例子出发，设两个向量![img](https://img-my.csdn.net/uploads/201304/03/1364953480_7616.jpg)和![img](https://img-my.csdn.net/uploads/201304/03/1364953493_7554.jpg)，而![img](https://img-my.csdn.net/uploads/201304/03/1364953575_7320.jpg)即是到前面说的五维空间的映射，因此映射过后的内积为：

![img](https://img-my.csdn.net/uploads/201304/03/1364953615_2896.jpg)