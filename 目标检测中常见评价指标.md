[TOC]

# 目标检测中常见评价指标

## 1.概述

- 准确率 （Accuracy）
- 混淆矩阵 （Confusion Matrix）
- 精确率（Precision）
- 召回率（Recall）
- 平均正确率（AP）
- mean Average Precision(mAP)
- 交除并（IoU）
- ROC + AUC
- 非极大值抑制（NMS）

## 2.精确度、召回率、F1-score

### 2.1 公式讲解

假设原始样本中有两类，其中： 
  1：总共有 P 个类别为 1 的样本，假设类别 1 为正例。 
  2：总共有 N 个类别为 0 的样本，假设类别 0 为负例。 
经过分类后： 
  3：有 TP 个类别为 1 的样本被系统正确判定为类别 1，FN 个类别为 1 的样本被系统误判定为类别 0，显然有 P=TP+FN； 

  4：有 FP 个类别为 0 的样本被系统误判断定为类别 1，TN 个类别为 0 的样本被系统正确判为类别 0，显然有 N=FP+TN； 

![è¿éåå¾çæè¿°](https://img-blog.csdn.net/20180906110246947?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lfeGlhbnNoZW5n/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

![img](https://images2018.cnblogs.com/blog/1393464/201805/1393464-20180522092210399-1308010205.png)

​		如上表格所示，模型的最终预测结果可分为上述四类情况，假设我们现在是要预测队伍里的男生女生人数，男生为正例，女生为负例，则：

1、True Positive：正确的正例，即男生被正确分为男生
2、False Positive：错误的正例，即女生被错误分为男生
3、False Negative：错误的负例，即男生被错误分为女生
4、True Negative：正确的负例，即女生被正确分为女生

​		**我们可以这样记忆，前面为True的就代表分类正确，False的就代表分类错误；后边是Positive的就是正例，后边是Negative的就是负例。**

下面是精确度(Precision)、召回率(Recall)和F1_Score的公式：

![è¿éåå¾çæè¿°](https://img-blog.csdn.net/20180906111249373?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lfeGlhbnNoZW5n/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)



> ### 精确度（Precision）:
>
> P = TP/(TP+FP);

​		分对的样本数除以所有的样本数 ，即：**准确（分类）率** = 正确预测的正反例数 / 总数。

　　**准确率一般用来评估模型的全局准确程度**，不能包含太多信息，无法全面评价一个模型性能。

​		反映了**被分类器判定的正例中真正的正例样本的比重**。

> ### 召回率（recall）:
>
> R = TP/(TP+FN) = 1 - FN/T;

​		**反映了被正确判定的正例占总的正例的比重。**

> ### F1值：
>
> ​       F1 = 2 * 召回率 * 准确率 / (召回率 + 准确率)；

### 2.2 规律总结

由此我们可以得到以下规律：

1. 假设模型分类取得了正例和负例的分类结果，那么Precision就是模型所得分类结果中，**真正的正例与分类所得正例的比值**，也可叫作查准率；

2. Recall就是模型所得分类结果中，**真正的正例与数据中实际真正正例的比值**，也可叫做查全率；

3. 在对模型进行评估的过程中，仅用精确度或者召回率去评估模型是无法全面评估模型优劣的，所以人们将**精确度和召回率结合起来**，得到了F1评分作为模型的实际评分准则，在多分类问题中，F1是精确度和召回率的**调和平均：**

   ![è¿éåå¾çæè¿°](https://img-blog.csdn.net/20180906112328707?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3lfeGlhbnNoZW5n/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

4. 最后提一下所谓的准确率(Accuracy)，准确率就是分类任务中**全部分类正确(不论正例负例)的总数与全体总数的比值**，Accuracy = (TP + TN )/( TP + FP + TN + FN)。

### 2.3 案例

​		在安防监控异常事件监测时，在模型训练好之后投入使用时，你可以调整阈值，使模型选择得分最高的一些场景报警，而忽略那些不太有把握的

​		在预测了100个场景之后，很可能模型只报警了10次，在这10次报警中有8次确实是危险场景，即正确报警(TP)，另外2次则是误报(FP)，即并没有危险事件发生，这时候查准率就会很高，为8/10=80%。

​		但是我们会发现，剩下90次未报警的场景中，会有很多得分偏低但确实有危险发生的场景被未报警，比如有30次漏掉(FN)的话，那查全率就是8/(8+30)=21.05%，这时就是**为了查准而放弃了很多可疑场景，导致查全率特别低。**

### 2.4 总结

​		所以，查准率和查全率往往是一对**矛盾**的量，想要保证查准率就需要舍弃一些查全率，在实际应用中，需要**设置好阈值**，也就是在“不要乱报”（得分很高时才报，拿不准的就放过，此时查准率高）和“宁可错杀一千，不可放过一个”（只要得分足够高，就列为嫌疑犯，此时查全率高）中做好权衡。

​		那有没有可能使这两者同时高呢？当然有可能，这取决于你模型的能力，设想下面几种情况：

​		假设你训练的模型是上帝，对所有的二分类问题都能准确的回答是或不是，也就是能做到Accuracy为100%。在这种情况下，就能保证查准率和查全率同为为100%。

​		此时，如果模型的能力略有降低，Accuracy降到95%左右，此时模型在对100个视频做了预测之后，报警35次，其中有33次是正确的(TP)，2次是误报，另外还有5次漏报，则此时查准率为33/35=94.3%，查全率为33/38=86.9。	

​		也就是说尽管二者是矛盾的，但他的表现和Accuracy有着必然的联系。如果你的模型预测能力很差，那么如果你想保证较高的查准率，这必须大幅度牺牲查全率，反过来也是一样，要想保证查全率，就要大幅牺牲查准率；

​		但如果你的模型预测能力还可以，那么二者相互牺牲的代价就不会那么大；一旦你的模型预测能力爆表，则相互牺牲的程度就更低，甚至可以使二者都很高。所以Accuracy是查准率和查全率的重要保证。

### 2.5 RP曲线

​		P-R曲线即 以 precision 和 recall 作为 纵、横轴坐标 的二维曲线，**P-R曲线围起来的面积就是AP值**，通常来说一个**越好的分类器，AP值越高**。

  		在目标检测中，每一类都可以根据 recall 和 precision绘制P-R曲线，AP就是该曲线下的面积，mAP就是所有类AP的平均值。

## 3.误报率、漏报率

​		在真正理解了这两个量之后，还有必要了解一下误报率（虚报率）和漏报率

> ### 虚警概率（False Alarm）
>
> FA = FP/(TP + FP) = 1–P；
> 反映有多少个**负例被误判。**
>
> 误报率=1-查准率

> ### 漏警概率（Missing Alarm）
>
> MA = FN/(TP + FN) = 1–TP/T = 1-R；
> 反映有多少个**正例被漏判。**
>
> **漏报率=1-查全率**

## 4.IoU

​		IoU(Intersection over Union) 这一值，可以理解为**系统预测出来的框**与**原来图片中标记的框**的重合程度。

​		IoU是一种测量在特定数据集中检测相应物体准确度的一个标准；在很多检测中都有用到这种方法，例如RCNN、Faster-RCNN、YOLO；

​		IoU是一个简单的测量标注，只要是在输出中得到一个**预测范围(Bounding Box)**的任务都可以用IoU进行测量，为了可以使IoU用于测量任意大小形状的物体检测：

​		计算方法即**检测结果Detection Result**与 **Ground Truth** 的交集比上它们的并集，即为检测的准确率： 

如下图所示： 
蓝色的框是：GroundTruth 
黄色的框是：DetectionResult 
绿色的框是：DetectionResult ⋂ GroundTruth 
红色的框是：DetectionResult ⋃ GroundTruth

![è¿éåå¾çæè¿°](https://img-blog.csdn.net/20170105154756543?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvaHlzdGVyaWMzMTQ=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

​		一般认为，IoU>0.5是好的结果，但是在实际应用中，大多数算法会设置IoU>0.7。

## 5.准确率 （Accuracy）

> A = (TP + TN)/(P+N) = (TP + TN)/(TP + FN + FP + TN);

​		反映了**分类器统对整个样本的判定能力——能将正的判定为正，负的判定为负。**

## 6.混淆矩阵 （Confusion Matrix）

​		混淆矩阵中的横轴是**模型预测的类别数量统计**，纵轴是**数据真实标签的数量统计。**

　　对角线，**表示模型预测和数据标签一致的数目**，所以**对角线之和除以测试集总数就是准确率。**

​		对角线上**数字越大越好**，在可视化结果中颜色越深，说明模型在该类的预测准确率越高。

​		如果按行来看，每行不在对角线位置的就是错误预测的类别。总的来说，我们希望对角线越高越好，非对角线越低越好。

## 7.平均精度（Average-Precision，AP）与 mean Average Precision(mAP)

​		AP就是Precision-recall 曲线下面的面积，通常来说一个越好的分类器，AP值越高。

　　mAP是多个类别AP的平均值。这个mean的意思是对每个类的AP再求平均，得到的就是mAP的值，mAP的大小一定在[0,1]区间，越大越好。

> **该指标是目标检测算法中最重要的一个。**

　　在正样本非常少的情况下，PR表现的效果会更好。

![img](https://images2018.cnblogs.com/blog/1393464/201806/1393464-20180606163413449-539246188.png)

## 8.ROC（Receiver Operating Characteristic）曲线与AUC（Area Under Curve）

![img](https://images2018.cnblogs.com/blog/1393464/201805/1393464-20180522114133617-572267222.png)

### 8.1 ROC曲线

- 横坐标：假正率(False positive rate， FPR)，FPR = FP / [ FP + TN] ，代表所有负样本中错误预测为正样本的概率，假警报率；
- 纵坐标：真正率(True positive rate， TPR)，TPR  = TP / [ TP + FN] ，代表所有正样本中预测正确的概率，命中率。
- 对角线对应于随机猜测模型，而（0,1）对应于所有整理排在所有反例之前的理想模型。
- 曲线越接近左上角，分类器的性能越好。

　　**ROC曲线有个很好的特性：**当测试集中的正负样本的分布变化的时候，ROC曲线能够保持不变。

​		在实际的数据集中经常会出现类不平衡（class imbalance）现象，即负样本比正样本多很多（或者相反），而且测试数据中的正负样本的分布也可能随着时间变化。

### 8.1.2 ROC曲线绘制

　　（1）根据每个测试样本属于正样本的概率值**从大到小**排序；

　　（2）从高到低，依次将“Score”值作为阈值threshold，当测试样本属于正样本的概率大于或等于这个threshold时，我们认为它为正样本，否则为负样本；

　　（3）每次选取一个不同的threshold，我们就可以得到一组FPR和TPR，即ROC曲线上的一点。 

 　　当我们将threshold设置为1和0时，分别可以得到ROC曲线上的(0,0)和(1,1)两个点。将这些(FPR,TPR)对连接起来，就得到了ROC曲线。当threshold取值越多，ROC曲线越平滑。

### 8.1.3 AUC

 　　AUC（Area Under Curve）即为ROC曲线下的面积。**AUC越接近于1，分类器性能越好。**

 　　**物理意义**：首先AUC值是一个**概率值**，当你随机挑选一个正样本以及一个负样本，当前的分类算法根据计算得到的Score值将这个正样本排在负样本前面的概率就是AUC值。

​		当然，AUC值越大，当前的分类算法越有可能将正样本排在负样本前面，即能够更好的分类。

　　**计算公式**：就是求曲线下矩形面积。

　![img](https://images2018.cnblogs.com/blog/1393464/201806/1393464-20180606173324079-1815794390.png)

## 9.PR曲线和ROC曲线比较

### 9.1 ROC曲线特点

- 优点
  1. 当测试集中的正负样本的分布变化的时候，**ROC曲线能够保持不变。**因为TPR聚焦于正例，FPR聚焦于与负例，使其成为一个比较均衡的评估方法。
  2. 在实际的数据集中经常会出现类不平衡（class imbalance）现象，即负样本比正样本多很多（或者相反），而且测试数据中的正负样本的分布也可能随着时间变化。
- 缺点
  1. 上文提到ROC曲线的优点是不会随着类别分布的改变而改变，但这在某种程度上也是其缺点。因为负例N增加了很多，而曲线却没变，这等于产生了大量FP。像信息检索中如果主要关心正例的预测准确性的话，这就不可接受了。
  2. 在类别不平衡的背景下，负例的数目众多致使FPR的增长不明显，导致ROC曲线呈现一个过分乐观的效果估计。
  3. ROC曲线的横轴采用FPR，根据FPR ，当负例N的数量远超正例P时，FP的大幅增长只能换来FPR的微小改变。结果是虽然大量负例被错判成正例，在ROC曲线上却无法直观地看出来。（当然也可以只分析ROC曲线左边一小段）

### 9.2 PR曲线

​		PR曲线使用了Precision，**因此PR曲线的两个指标都聚焦于正例**。

​		类别不平衡问题中由于主要关心正例，所以在此情况下**PR曲线被广泛认为优于ROC曲线。**

### 9.3 使用场景

​		ROC曲线由于兼顾正例与负例，所以适用于**评估分类器的整体性能**，相比而言**PR曲线完全聚焦于正例。**
​		如果有多份数据且存在不同的类别分布，比如信用卡欺诈问题中每个月正例和负例的比例可能都不相同，这时候如果只想单纯地比较分类器的性能且剔除类别分布改变的影响，则ROC曲线比较适合，因为类别分布改变可能使得PR曲线发生变化时好时坏，这种时候难以进行模型比较；反之，如果想测试不同类别分布下对分类器的性能的影响，则PR曲线比较适合。
​		**如果想要评估在相同的类别分布下正例的预测情况，则宜选PR曲线。**
​		类别不平衡问题中，ROC曲线通常会给出一个乐观的效果估计，所以大部分时候还是PR曲线更好。

​		最后可以根据具体的应用，在曲线上找到最优的点，得到相对应的precision，recall，f1 score等指标，去调整模型的阈值，从而得到一个符合具体应用的模型。

## 10.非极大值抑制（NMS）

​		Non-Maximum Suppression就是需要根据**score矩阵**和**region的坐标信息**，从中找到**置信度比较高**的bounding box。对于有重叠在一起的预测框，只保留得分最高的那个。

　　（1）NMS计算出每一个bounding box的面积，然后根据score进行排序，把score最大的bounding box作为队列中首个要比较的对象；

　　（2）计算其余bounding box与当前最大score与box的IoU，去除IoU大于设定的阈值的bounding box，保留小的IoU得预测框；

　　（3）然后重复上面的过程，直至候选bounding box为空。

　　最终，检测了bounding box的过程中有两个阈值，一个就是IoU，另一个是在过程之后，从候选的bounding box中剔除score小于阈值的bounding box。

​		需要注意的是：Non-Maximum Suppression一次处理一个类别，如果有N个类别，Non-Maximum Suppression就需要执行N次。

