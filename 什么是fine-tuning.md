[TOC]

# 什么是fine-tuning

## 1.概念

​		在实践中，由于数据集不够大，很少有人从头开始训练网络。常见的做法是使用预训练的网络（例如在ImageNet上训练的分类1000类的网络）来重新fine-tuning（也叫微调），或者当做特征提取器。

## 2.常见迁移学习场景

1. **卷积网络当做特征提取器。**

   ​		使用在ImageNet上预训练的网络，去掉最后的全连接层，剩余部分当做特征提取器（例如AlexNet在最后分类器前，是4096维的特征向量）。

   ​		这样提取的特征叫做CNN codes。得到这样的特征后，可以使用线性分类器（Liner SVM、Softmax等）来分类图像。

2. **Fine-tuning卷积网络。**

   ​		替换掉网络的输入层（数据），使用新的数据继续训练。Fine-tune时可以选择fine-tune全部层或部分层。

   ​		通常，**前面的层提取的是图像的通用特征**（generic features）（例如边缘检测，色彩检测），这些特征对许多任务都有用。

   ​		**后面的层提取的是与特定类别有关的特征，**因此fine-tune时常常只需要Fine-tuning后面的层。

## 3.预训练模型

​		在ImageNet上训练一个网络，即使使用多GPU也要花费很长时间。因此人们通常共享他们预训练好的网络，这样有利于其他人再去使用。例如，Caffe有预训练好的网络地址Model Zoo。

## 4.何时以及如何Fine-tune

​		决定如何使用迁移学习的因素有很多，这是最重要的只有两个：

- 新数据集的大小
- 以及新数据和原数据集的相似程度。

​		有一点一定记住：**网络前几层学到的是通用特征，后面几层学到的是与类别相关的特征。**

​		这里有使用的四个场景：

1、新数据集比较小且和原数据集相似。

​		因为新数据集比较小，如果fine-tune可能会过拟合；又因为新旧数据集类似，我们期望他们高层特征类似，可以使用预训练网络当做特征提取器，用提取的特征训练线性分类器。

2、新数据集大且和原数据集相似。

​		因为新数据集足够大，可以fine-tune整个网络。

3、新数据集小且和原数据集不相似。

​		新数据集小，最好不要fine-tune，和原数据集不类似，最好也不使用高层特征。这时可是使用前面层的特征来训练SVM分类器。

4、新数据集大且和原数据集不相似。

​		因为新数据集足够大，可以重新训练。但是实践中fine-tune预训练模型还是有益的。新数据集足够大，可以fine-tine整个网络。

## 5.实践建议

1. 预训练模型的限制。

​		使用预训练模型，受限于其网络架构。例如，你不能随意从预训练模型取出卷积层。但是因为参数共享，可以输入任意大小图像；卷积层和池化层对输入数据大小没有要求（只要步长stride fit），其输出大小和属于大小相关；全连接层对输入大小没有要求，输出大小固定。

​       2.学习率。

​		与重新训练相比，fine-tune要使用更小的学习率。因为训练好的网络模型权重已经平滑，我们不希望太快扭曲（distort）它们（尤其是当随机初始化线性分类器来分类预训练模型提取的特征时）。