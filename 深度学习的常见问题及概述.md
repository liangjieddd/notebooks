[TOC]

# 深度学习

## 1.为什么用神经网络

​		对于**非线性分类**问题，如果用多元线性回归进行分类，需要构造许多高次项，导致特征特多，学习参数过多，从而复杂度太高 。

　　在神经网络中引入激活函数一个重要的原因就是为了引入非线性。

## 2.CNN基本问题

层级结构：输入层->卷积层->激活层->池化层->卷积层->激活层->池化层->全连接层····

**（1）输入层数据预处理：**去均值；归一化；PCA/白化；

　　**去均值：**即0均值化，CNN常用，训练集所有像素值减去均值，把输入数据各个维度中心化到0，测试集也减相同的均值；

　　　　　　目的：（1）数据有过大的均值可能导致参数的梯度过大，在梯度回传时会有一些影响；（2）如果有后续的处理，可能要求数据0均值，比如PCA。

　　**归一化：**幅度归一化到同样的范围；

　　　　　　目的：为了让不同维度的数据具有相同的分布规模，方便操作，图像一般不需要。

　　**PCA/白化：**降维，白化是对PCA降维后的数据每个特征轴上的幅度归一化；

　　　　　　目的：相当于在零均值化和归一化操作之间插入一个旋转操作，将数据投影在主轴上。图像一般不需要，因为图像的信息本来就是依靠像素之间的相对差异来体现的。

 　　**（2）池化层作用：**

　　本质上，是在精简feature map数据量的同时，最大化保留空间信息和特征信息，的处理技巧；

　　目的是，（a）对feature map及参数进行压缩，起到降维作用；

　　　　　　（b）减小过拟合的作用。包括Max pooling 和average pooling；

　　　　　　（c）引入不变性，包括平移、旋转、尺度不变性。但CNN的invariance的能力，本质是由convolution创造的；

　　简而言之，如果输入是图像的话，那么池化层的最主要作用就是**压缩图像**。

　　**为什么不用卷积步长的压缩：**因为pooling layer的工作原理，在压缩上比convolution更专注和易用。

　　**为什么不用BP神经网络去做呢？**

　　（1）全连接，权值太多，需要很多样本去训练，计算困难。**应对之道：**减少权值的尝试，局部连接，权值共享。

　　卷积神经网络有两种神器可以降低参数数目。
　　第一种神器叫做**局部感知野**，一般认为人对外界的认知是从局部到全局的，而图像的空间联系也是局部的像素联系较为紧密，而距离较远的像素相关性则较弱。因而，每个神经元其实没有必要对全局图像进行感知，只需要对局部进行感知，然后在更高层将局部的信息综合起来就得到了全局的信息。　　

　　第二级神器，即**权值共享**。

　　（2）边缘过渡不平滑。**应对之道：**采样窗口彼此重叠。

**3、调参优化方法：基本原则—快速试错**

　　**（1）由小数据到大数据：**刚开始, 先上小规模数据, 模型往大了放, 只要不爆显存, 能用256个filter你就别用128个. 直接奔着过拟合去。

　　　　**目的：**验证自己的训练脚本的流程对不对。如果小数据量下, 你这么粗暴的大网络奔着过拟合去都没效果. 那么, 你要开始反思自己了, 模型的输入输出是不是有问题? 要不要检查自己的代码(永远不要怀疑工具库, 除非你动过代码)? 模型解决的问题定义是不是有问题? 你对应用场景的理解是不是有错?

　　**（2）loss设计要合理**

　　一般来说分类就是Softmax, 回归就是L2的loss. 但是要注意loss的错误范围(主要是回归), 你预测一个label是10000的值, 模型输出0, 你算算这loss多大, 这还是单变量的情况下. 一般结果都是nan. 所以不仅仅**输入要做normalization, 输出也要这么弄.**

　　多任务情况下, 各loss想法**限制在一个量级**上, 或者最终限制在一个量级上, 初期可以着重一个任务的loss。

　　**（3）观察loss胜于观察准确率**

　　LOSS下降时稳定的，而准确率有时是突变的，不能反映真实情况。给NN一点时间, 要根据任务留给NN的学习一定空间. 不能说前面一段时间没起色就不管了. 有些情况下就是前面一段时间看不出起色, 然后开始稳定学习.

**（4）确认分类网络学习充分**

　　分类网络就是学习类别之间的界限. 你会发现, 网络就是**慢慢的从类别模糊到类别清晰的**. 怎么发现? 看Softmax输出的概率的分布. 如果是二分类, 你会发现, 刚开始的网络预测都是在0.5上下, 很模糊. 随着学习过程, 网络预测会慢慢的移动到0,1这种极值附近. 所以, 如果你的网络预测分布靠中间, 再学习学习。

　　**（5）学习率设置合理**

　　太大: loss爆炸, 或者nan；太小: 半天loss没反映。当loss在当前LR下一路降了下来, 但是半天不再降了，就需要进一步降低了LR了。

　　**（6）对比训练集和验证集的loss**

　　判断过拟合, 训练是否足够, 是否需要early stop的依据, 这都是中规中矩的原则, 不多说了.

　　**（7）清楚receptive field的大小**

 　　CV的任务, **context window是很重要的**. 所以你对自己模型的receptive field的大小要心中有数. 这个对效果的影响还是很显著的. 特别是用FCN, 大目标需要很大的receptive field. 不像有fully connection的网络, 好歹有个fc兜底, 全局信息都有。

　　**（8）最后一层激活函数：**分类softmax，回归的话一般不用，直接输出wx+b。

　　**（9）训练数据增强：**旋转、裁剪、亮度、色度、饱和度等变化等增加鲁棒性。

**简短的注意事项：**

　　（1）预处理: **-mean/std zero-center**就够了, PCA, 白化什么的都用不上. 我个人观点, 反正CNN能学习encoder, PCA用不用其实关系不大, 大不了网络里面自己学习出来一个.

　　（2）训练数据要shuffle, shuffle, shuffle。

　　（3）**Dropout**, Dropout, Dropout(不仅仅可以防止过拟合, 其实这相当于做人力成本最低的**Ensemble**, 当然, 训练起来会比没有Dropout的要慢一点, 同时网络参数你最好相应加一点, 对, 这会再慢一点)。注意，一般取值0.5，在测试时关掉。

　　（4）CNN更加适合训练回答**是否的问题**, 如果任务比较复杂, 考虑先用分类任务训练一个模型再finetune.
　　（5）不知选什么就用：激活函数用ReLU(CV领域).；参数初始化用xavier；

　　（6）LRN一类的, 其实**可以不用.** 不行可以再拿来试试看.

　　（7）filter数量2^n。**第一层的filter, 数量不要太少. 否则根本学不出来(底层特征很重要).**
　　（8）多尺度的图片输入(或者网络内部利用多尺度下的结果)有很好的提升效果。

　　（9）sgd adam 这些选择上, 看你个人选择. 一般对网络不是决定性的. 反正我无脑用sgd + momentum。

　（10）**shortcut**的联接是有作用的。

　　（11）batch normalization我一直没用, 虽然我知道这个很好, 我不用仅仅是因为我懒. 所以要鼓励使用batch normalization。**好坏另说，试了才知道。**

　　（12）不要完全相信论文里面的东西. 结构什么的觉得可能有效果, 可以拿去试试.

　　（13）暴力调参最可取, 毕竟, 自己的生命最重要. 你调完这个模型说不定过两天这模型就扔掉了.

 