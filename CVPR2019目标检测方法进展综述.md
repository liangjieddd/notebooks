[TOC]

# CVPR2019目标检测方法进展综述

## 1.前言

​		目标检测是很多计算机视觉应用的基础，比如实例分割、人体关键点提取、人脸识别等，它结合了**目标分类**和**定位**两个任务。

​		现代大多数目标检测器的框架是 **two-stage**，其中目标检测被定义为一个多任务学习问题：

1）区分前景物体框与背景并为它们分配适当的类别标签；

2）回归一组系数使得**最大化**检测框和目标框之间的交并比（IoU）或其它指标。最后，通过一个 NMS 过程移除冗余的边界框（**对同一目标的重复检测**）。

​		本文首先综述近年来二维目标检测的优化方向，之后介绍CVPR2019目标检测最新进展，包括优化IoU的GIoU，优化anchor设计的GA-RPN，以及single-stage detection的FSAF。

## 2.二维目标检测的优化方向

二维目标检测实现和优化方向包括：

- backbone
- IoU
- 损失函数
- NMS
- anchor
- one shot learning/zero shot learning

### 2.1 基于目标检测的backbone和特征提取

​		目标检测的backbone一般**基于ImageNet预训练的图像分类网络**。图像分类问题只关注分类和感受视野，不用关注物体定位，但是目标检测领域同时很关注空间信息。

​		如果下采样过多，会导致最后的feature map很小，小目标很容易漏掉。很多基础架构网络，比如ResNet、Xception、DenseNet、FPN、DetNet、R-CNN，PANet、等神经网络提取图像的上下文信息，不断在特征提取方向优化。

### 2.2 基于优化的算法

​		包括UnitBox，IoU-Net[1]，旷视科技ECCV2018有一篇论文是引入IoU-Net，其能**预测检测到的边界框和它们对应的真实目标框之间的 IoU**，使得该网络能像其分类模块一样，对检测框的定位精确程度有所掌握，神经网络在Backbone引入IoU-Net做边界修订。

### 2.3 基于优化损失函数的方法

​		包括L1和L2，Focal loss等。

### 2.4 基于优化NMS的方法

​		包括Soft-NMS,Softer-NMS,以及Relation Netwrok，ConvNMS，NMS Network，Yes-Net等，详细可参看本人一篇笔记《目标检测算法中检测框合并策略技术综述》[2]。

### 2.5 基于Anchor生成的算法

​		比如Sliding window、Region Proposal Network (RPN) 、CornerNet、meta-anchor等。  

### 2.6 one-shot learning以及zero shot learning

​		都属于**迁移学习**领域主要研究的是**网络少样本精准分类问题**，**单样本学习能力**。CVPR2019有一篇基于one-shot learning[7]，值得关注。



## 3.GIoU

### 3.1 Motivation

​		在目标检测的评价体系中，有一个参数叫做 IoU (Intersection over Union)，简单来讲就是模型产生的目标窗口和原来标记窗口的交叠率。

​		具体我们可以简单的理解为： 即检测结果(DetectionResult)与 Ground Truth 的交集比上它们的并集，即为检测的准确率 IoU :

 

![img](https://mmbiz.qpic.cn/mmbiz_png/75DkJnThACnWkR6ASlRBAgh4iabbYibQHZ2icEpuqh5SibJxyGLsiaZq3iatfZEljeib1G3L6YtvozxQ7Wx2mAf8wH0sw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

​                                           

​		IoU 是目标检测领域最重要的**评价尺度**之一，特性是**对尺度不敏感**，主要判断检测框的重合程度。但是对于CNN而言，没有方向信息，无法反馈神经网络的边界回归框应该如何调整。既直接用IoU作为损失函数会出现两个问题：

1. 如果两个框没有相交，根据定义，IoU=0，不能反映两者的距离大小（重合度）。同时因为loss=0，没有梯度回传，无法进行学习训练。
2. IoU无法精确的反映两者的重合度大小。如图 1所示，IoU的变化无法反馈定位框的重合度和调整方向。

​		针对IoU上述两个缺点，本文提出一个新的指标generalized IoU[3]（GIoU）如图 2所示。

### 3.2 损失函数设计

​		GIoU的定义很简单，就是先计算两个框的最小闭包区域面积，再计算IoU，再计算闭包区域中不属于两个框的区域占闭包区域的比重，最后用IoU减去这个比重得到GIoU。

GIoU有如下5个特点：

1. 与IoU相似，GIoU也是一种距离度量,满足损失函数的基本要求，loss可以用 ![[公式]](https://www.zhihu.com/equation?tex=L_%7BGIoU%7D+%3D+1+-+GIoU) 来计算

2. 同原始IoU类似，GIoU对scale不敏感

3. **GIoU是IoU的下界**，在两个框无线重合的情况下，IoU=GIoU

4. GIoU总是小于等于IoU，IoU取值[0,1]，但GIoU有对称区间，取值范围[-1,1]。在两者重合的时候取最大值1，在两者无交集且无限远的时候取最小值-1，因此GIoU是一个非常好的距离度量指标。

5. 与IoU只关注重叠区域不同，GIoU不仅关注重叠区域，还关注其他的非重合区域，能更好的反映两者的重合度。由于GIoU引入了包含A，B两个形状的C，所以当A，B不重合时，依然可以进行优化。

   ​		总之就是保留了IoU的原始性质同时弱化了它的缺点。于是论文认为可以将其作为IoU的替代。

![img](https://mmbiz.qpic.cn/mmbiz_png/75DkJnThACnWkR6ASlRBAgh4iabbYibQHZSJCLpE6ribG3aTx1Jow3EtQibnVQibpYjgWqcgXRiaRlqcAzXPcqd8MtZw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

​                                                      图 1 IoU和GIoU评价对比

​                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           

![img](https://mmbiz.qpic.cn/mmbiz_png/75DkJnThACnWkR6ASlRBAgh4iabbYibQHZm8icnz0FsY7qzGND3Aia9dCKNwkjmVCNcLwuqhd5toVOhibftlGBpyyzA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

​                                                                     图 2 GIoU计算过程

### 3.3 优缺点分析

​		GIoU loss可以替换掉大多数目标检测算法中bounding box regression，本文选取了Faster R-CNN、Mask R-CNN和YOLO v3 三个方法验证GIoU loss的效果。

​		可以看出YOLOv3在COCO数据集有明显优势，但在其他模型下优势不明显，作者也指出了Faster rcnn和mask rcnn效果不明显的原因是anchor很密，比如 Faster rcnn 2k个Anchor Boxes,各种类型覆盖应有尽有，不仅仅是根据IoU和NMS挑选合适的检测框，而且需要对检测框进行有方向的修订。

​		总体来说，文章的motivation比较好，指出用L1、L2作为regression损失函数的缺点，以及用直接指标IoU作为损失函数的缺陷性，提出新的metric来代替L1、L2损失函数，从而提升regression效果，想法简单粗暴，但相比state-of-art没有明显的性能优势。

## 4 GA-RPN

​		GA-RPN[4]是香港中文大学，商汤和亚马逊联合提出，在COCO Challenge 2018 检测任务的冠军方法中，在极高的 baseline 上涨了1个点。

​		GA-RPN（Guided Anchoring）是一种新的 anchor 生成方法，即通过图像特征来指导anchor 的生成。通过CNN预测 anchor 的位置和形状，生成稀疏而且形状任意的 anchor，并且设计Feature Adaption 模块来修正特征图使之与 anchor精确匹配。GA-RPN相比RPN减少90%的anchor，并且提高9.1%的召回率，将其用于不同的物体检测器Fast R-CNN, Faster R-CNN and RetinaNet，分别提高 检测mAP 2.2%,2.7% ,1.2%。

## 5 Motivation

​		Anchor 是目标检测中的一个重要概念，通常是人为设计的一组框，作为分类（classification）和框回归（bounding box regression）的基准框。基于无论是单阶段（single-stage）检测器还是两阶段（two-stage）检测器，都广泛地使用了 anchor。

​		常见的生成 anchor 的方式是滑窗（sliding window）和RPN（region proposal network），two-stage 基于RPN首先定义 k 个特定尺度（scale）和长宽比（aspect ratio）的 anchor，single-stage 使用sliding window在特征图上以一定的步长滑动。这种方式在 Faster R-CNN，SSD，RetinaNet 等经典检测方法中被广泛使用。

​		基于RPN和sliding window的anchor生成方式有两个缺点：（1）anchor的尺度和长宽比需要预先定义，针对不同类型的检测任务需要调整这些超参数，预先定义好的 anchor 形状不一定能满足极端大小或者长宽比悬殊的物体。(2)为了保证召回率，需要生成密集的anchor，引入过多负样本同时影响模型的速率。

​		在一般拍摄图像中，一般检测目标是不均匀分布且稀疏的。检测目标的尺度和图像内容、位置和几何形状相关。基于图像的先验知识，论文提出稀疏的anchor生成方式：首先生成可能包含目标的子区域中心位置，然后在图像不同位置确定尺度和长宽比，既稀疏，形状根据位置可变的 anchor。



论文提出了anchor的设计两个要求

1. alignment，为了用卷积特征作为anchor的表示，anchor的中心需要和特征图的像素中心比较好地对齐
2. consistency，不同位置(不同卷积层)对应的anchor的形状和大小应该一致。



## 6 网络架构

​		CornerNet模型预测目标边界框的左上角和右下角一对顶点，既使用单一卷积模型生成热点图和连接矢量,而论文提出的GA-RPN，直接预测anchor 的位置和形状（长宽）。生成anchor过程可以分解为两个步骤，anchor 位置预测和形状预测。

​		如图 3所示GA-RPN，其backbone基于FPN,而Guided anchoring生成anchor。Guided anchoring包括两个并行的分支：anchor Generation分别预测 anchor 位置和形状，然后结合在一起得到 anchor。Feature Adaption 模块进行 anchor 特征的调整，得到新的特征图提供之后的预测（anchor 的分类和回归）使用。

![img](https://mmbiz.qpic.cn/mmbiz_png/75DkJnThACnWkR6ASlRBAgh4iabbYibQHZV0Qm4hp6ticxSdZ6HsPDjz0c8PMaE9UjB1dLITDKMUx2GhbEcEWEnyw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

​																	图 3 GA-RPN网络架构



**位置预测**

​		位置预测分支的目标是预测那些区域应该作为中心点来生成 anchor，是一个二分类问题，预测是不是物体的中心。

​		FPN特征图经过位置预测子网络生成位置得分图。由1x1卷积和sigmoid激活函数组成。更深的卷积网络可以实现更精确的位置信息，但是1x1卷积实现效率和精确度的平衡。通过位置得分图阈值的过滤，可以舍弃90%的anchor同时保持较高的召回率。

​		我们将整个 feature map 的区域分为物体中心区域，外围区域和忽略区域，大概思路就是将 ground truth 框的中心一小块对应在 feature map 上的区域标为物体中心区域，在训练的时候作为正样本，其余区域按照离中心的距离标为忽略或者负样本。通过位置预测，我们可以筛选出一小部分区域作为 anchor 的候选中心点位置，使得 anchor 数量大大降低。在 inference 的时候，预测完位置之后，我们可以采用 masked conv 替代普通的 conv，只在有 anchor 的地方进行计算，可以进行加速。



**形状预测**

​		针对每个检测出的中心点，设计一个最佳的anchor box。最佳anchor box的定义为：与预测的候选中心点的邻近ground truth box产生最大IOU的anchor box。

​		形状预测分支的目标是给定 anchor 中心点，预测最佳的长和宽，这是一个回归问题。论文直接使用 IoU 作为监督，来学习 w 和 h。无法直接计算w 和 h，而计算 IoU 又是可导的操作，所有使用基于bounded IoU Loss网络优化使得 IoU 最大。

​		作者认为，直接预测anchor box的宽高的话，范围太广不易学习，故将宽高值使用指数及比例缩放(ω =σ·s·edw ,h=σ·s·hdw )进行压缩，将搜索范围从[0，1000]压缩至[-1,1]。



**Feature Adaption**

​		由于每个位置的anchor形状不一样，所以不能直接利用F_I进行1x1的卷积预测每个anchor的结果，而应该对feature map进行adaption，也就是大一点的anchor对应的感受野应该大一点，小一点的anchor对应的感受野应该小一点，于是作者想到用可变形卷积的思想。先对每个位置预测一个卷积的offset（1x1卷积，输入为shape prediction），然后根据该offset field进行3x3的可变形卷积就完成了对feature map的adaption。通过这样的操作，达到了让 feature 的有效范围和 anchor 形状更加接近的目的，同一个 conv 的不同位置也可以代表不同形状大小的 anchor了。

 

## 优缺点分析

**优点：**

1、论文提出anchor设计的两个准则：alignment 和 consistency，指导基于anchor优化的方向。采用位置预测和形状预测两个分支，不需要像FPN预先设置尺度和长宽比，同时使用可变形卷积对feature map调整，生成高质量低密度的proposal，提高IoU的阈值进行训练。

2、提出了一种新的anchor策略，用于产生稀疏的任意形状的anchor；

3、论文提出的GA-RPN可以完全替代RPN，在Fast R-CNN, Faster R-CNN and RetinaNet等模型基础上提高目标检测模型的精度。



**缺点：**

1、论文假设图像中的目标是稀疏的。如果是稠密图像，比如车站或广场的拥挤人群，检测效果有待检验。

2、每一个点只产生一个anchor，那么对于那些目标中心重合，即一个点需要负责检测两个目标，似乎无法处理。

3、采用deformable卷积会相对地降低速度，同时根据DCN v2的分析，在deformable卷积中加入可调节的机制可能会更好。


  